{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7dc964",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "\n",
    "To run the analysis without dependency issues, please use Python 3.11.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a17798",
   "metadata": {},
   "source": [
    "### 1. Install libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas==1.5.3\n",
    "!pip3 install xgboost==3.0.1\n",
    "!pip3 install scikit-learn==1.2.2\n",
    "!pip3 install shap==0.47.2\n",
    "!pip3 install matplotlib==3.7.1\n",
    "!pip3 install tables==3.8.0\n",
    "!pip3 install gensim==4.3.0\n",
    "!pip3 install numba==0.57.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b7f77",
   "metadata": {},
   "source": [
    "### 2. Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa296098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model:\n",
    "\n",
    "best_model_2 = joblib.load('output/best_model_2.pkl')\n",
    "best_params = best_model_2.get_params() # Save the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad64c71",
   "metadata": {},
   "source": [
    "### 3. Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de780a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/justina/Desktop/Data Science/retirement_data_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes.value_counts()) # 3 columns are numerical, and upon further inspection, some of them are not needed.\n",
    "num_columns = data.select_dtypes(include=['float64'])\n",
    "print(num_columns)\n",
    "\n",
    "# remove these columns that are not needed:\n",
    "data = data.drop(columns=[\"exrate\", \"ex009age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d69d9",
   "metadata": {},
   "source": [
    "### 4. XGBoost\n",
    "#### Relevant links:\n",
    "\n",
    "| Note                         | Link                                        |\n",
    "|------------------------------|----------------------------------------------------|\n",
    "| Working with categorical variables |[Tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html) |\n",
    "| Information about XGBRegressor and possible parameters |[Documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#)     |\n",
    "| Hyperparameter tuning - RandomizedSearchCV |[Tutorial](https://dev.to/uche_4rm_germany/grid-and-randomized-hyperparameter-optimization-for-xgboost-algorithms-159k) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacc1c5",
   "metadata": {},
   "source": [
    "#### 4.1 Prepare the data - separate the target from the rest of predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"age_ret\", \"mergeid\"], axis=1)  # dropping the age of retirement and ID from the data, as it shouldn't have an inherent meaning to the prediction of age.\n",
    "y = data[\"age_ret\"] # contains only the column of retirement age\n",
    "\n",
    "X.shape # 212 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(\"category\") # Setting the column type to be \"categorical\" for all predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['ph012_'] = pd.to_numeric(X['ph012_'], errors='coerce') # Changing back weight column to be numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e6eb5",
   "metadata": {},
   "source": [
    "#### 4.2 Split the data into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec28f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec421d",
   "metadata": {},
   "source": [
    "#### 4.3 Prepare the parameter distribution for the hyperparameter tuning/search:\n",
    "\n",
    "Only run the code for sections 4.3 - 4.6 if the model and shap files are not loaded (the ones after loading the libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': randint(3, 8),\n",
    "    'learning_rate': uniform(0.005, 0.15),\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'subsample': uniform(0.2, 0.5),\n",
    "    'colsample_bytree': uniform(0.2, 0.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4e1f2",
   "metadata": {},
   "source": [
    "#### 4.4 Create a regressor, which will explore parameter values that fit the data the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5575c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = XGBRegressor(\n",
    "    tree_method = \"hist\", # Required for categorical support\n",
    "    enable_categorical = True,       # Enables native categorical handling\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2b598",
   "metadata": {},
   "source": [
    "#### 4.5 Perform the Randomized search [run time ~ 25 mins]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952c0c0",
   "metadata": {},
   "source": [
    "The code below was run once, then it is not needed as we load the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    estimator = reg_model, \n",
    "    param_distributions = params, \n",
    "    cv = 5, \n",
    "    n_iter = 100,\n",
    "    random_state = 42)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line below is run only once - after the random search is completed - to save the best model.\n",
    "\n",
    "#best_model = random_search.best_estimator_\n",
    "\n",
    "#best_model_2 = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line below saves the model - only needed to run once.\n",
    "\n",
    "#joblib.dump(best_model, 'output/best_model.pkl')\n",
    "#joblib.dump(best_model_2, 'output/best_model_2.pkl') # This is the second model that we use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d76a8e",
   "metadata": {},
   "source": [
    "#### 4.6 Plotting the mean test score across the different parameter settings/models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f4c5b",
   "metadata": {},
   "source": [
    "The code below requires the search to be done again. But the plot is already save in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6368731",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = pd.DataFrame(random_search.cv_results_).sort_values(\"rank_test_score\") # dataset to store the results\n",
    "\n",
    "# plot the output of the randomized search:\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(sorted_results[\"rank_test_score\"], \n",
    "         sorted_results[\"mean_test_score\"], \n",
    "         marker='o',\n",
    "         markersize=3,\n",
    "         linewidth=0.5,\n",
    "         color = \"steelblue\")\n",
    "plt.xlabel(\"Parameter rank\")\n",
    "plt.ylabel(\"Average test score\")\n",
    "plt.title(\"Model performance\")\n",
    "plt.grid(True)\n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/m_performance_par_rank_2.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# The top score is 0.185736."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a990fe",
   "metadata": {},
   "source": [
    "### 5. Predicting the data - full model with all features in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = best_model.predict(X_test)\n",
    "y_pred = best_model_2.predict(X_test)\n",
    "\n",
    "# Calculates the metrics:\n",
    "\n",
    "RMSE_full = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "R2_full = r2_score(y_test, y_pred)\n",
    "print(\"RMSE:\", RMSE_full)\n",
    "print(\"R²:\", R2_full)\n",
    "\n",
    "#RMSE: 4.813924345110674\n",
    "#R²: 0.22843654308677708"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f18c1a",
   "metadata": {},
   "source": [
    "### 6. Calculate feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dddc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recoding the names of the features:\n",
    "rename_dict = {\n",
    "    'dn042_': 'Gender [DN]',\n",
    "    'it004_': 'Recent internet use [IT]',\n",
    "    'hc116d3': 'Private long-term health insurance [HC]',\n",
    "    'country': 'Country',\n",
    "    'hc116dno': 'Other long-term health insurance [HC]',\n",
    "    'br039_': 'Alcohol use in last 7 days [BR]',\n",
    "    'ph048d9': 'Difficulty with heavy-lifting (5kg) [PH]',\n",
    "    'ph089dno': 'Not bothered by physical weakness [PH]',\n",
    "    'ph048dno': 'Not difficulties in everyday activities [PH]',\n",
    "    'ph012_': 'Weight of respondent [PH]',\n",
    "    'ph084_': 'Trouble with pain [PH]',\n",
    "    'ph089d4': 'Bothered by fatigue [PH]',\n",
    "    'hc010_': 'Dentist appointment in last 12 months[HC]',\n",
    "    'ph006d18': 'Emotional disorders [PH]',\n",
    "    'hc884_': 'Vaccination for flu [HC]',\n",
    "    'dn002_': 'Month of birth [DN]', \n",
    "    'ex001_': '(Not relevant) [EX]',\n",
    "    'hc602_': 'Number of doctor visits [HC]', \n",
    "    'ex009_': 'Life expectancy [EX]',\n",
    "    'ph004_': 'Long-term illness [PH]',\n",
    "    'ph003_': 'Health in general [PH]',\n",
    "    'br026_': 'Consumption of dairy products per week [BR]',\n",
    "    'ac012_': 'Life satisfaction [AC]'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18bb815",
   "metadata": {},
   "source": [
    "These two chunks below allow us to compare the feature importances when importances asre based on \"gain\", and compare how they are based on their \"weight\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below exports the gain measure:\n",
    "\n",
    "booster = best_model_2.get_booster()\n",
    "gain_score = booster.get_score(importance_type = 'gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4126ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df = pd.DataFrame.from_dict(gain_score, orient='index', columns=['Gain'])\n",
    "gain_df.index.name = 'Feature'\n",
    "gain_df.reset_index(inplace=True)\n",
    "\n",
    "gain_df['Feature'] = gain_df['Feature'].replace(rename_dict)\n",
    "\n",
    "# Sort and take top 10\n",
    "top_gain_df = gain_df.sort_values(by='Gain', ascending=False).head(15)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(top_gain_df['Feature'], top_gain_df['Gain'], color='lightsteelblue')\n",
    "plt.xlabel('Gain score')\n",
    "plt.title('Top 15 features by gain importance (XGBoost)')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.gca().invert_yaxis() \n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/feature_importances_top15_m2_gain.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77942876",
   "metadata": {},
   "source": [
    "Now, we calculate weight-based feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9daf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates weight-based, scaled feature importances:\n",
    "\n",
    "importances = best_model_2.feature_importances_\n",
    "\n",
    "feature_importance_data = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "feature_importance_data = feature_importance_data.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161147b4",
   "metadata": {},
   "source": [
    "These will be used for extracting the features, and later - plotting them. These turn out to be the same as importances based on \"gain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b83a8",
   "metadata": {},
   "source": [
    "#### 7.1 Extracting top 15 features [for comparison]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415024d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top 15 most important features?\n",
    "\n",
    "top_15_features = feature_importance_data.head(15)['Feature'].tolist()\n",
    "\n",
    "# Creating a subset of features for training and testing:\n",
    "\n",
    "X_train_top15 = X_train[top_15_features]\n",
    "X_test_top15 = X_test[top_15_features]\n",
    "\n",
    "# Model fit:\n",
    "\n",
    "best_model_2.fit(X_train_top15, y_train)\n",
    "\n",
    "# Predicion:\n",
    "\n",
    "y_pred_top15 = best_model_2.predict(X_test_top15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3b993",
   "metadata": {},
   "source": [
    "#### 7.2 Extracting top 10 features [for comparison]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb2bb5",
   "metadata": {},
   "source": [
    "Once the training and testing sets are adjusted, we use the model with the optimal hyperparameters (the one we found above), and train it on the most informative features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_2 = joblib.load('output/best_model_2.pkl') # Loads the model again just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top 10 most important features?\n",
    "\n",
    "top_10_features = feature_importance_data.head(10)['Feature'].tolist()\n",
    "\n",
    "X_train_top10 = X_train[top_10_features]\n",
    "X_test_top10 = X_test[top_10_features]\n",
    "\n",
    "best_model_2.fit(X_train_top10, y_train)\n",
    "\n",
    "y_pred_top10 = best_model_2.predict(X_test_top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde8d6f",
   "metadata": {},
   "source": [
    "#### 7.3 Calculate the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0afbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_top_15 = np.sqrt(mean_squared_error(y_test, y_pred_top15))\n",
    "R2_top_15 = r2_score(y_test, y_pred_top15)\n",
    "print(\"RMSE:\", RMSE_top_15)\n",
    "print(\"R²:\", R2_top_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_top_10 = np.sqrt(mean_squared_error(y_test, y_pred_top10))\n",
    "R2_top_10 = r2_score(y_test, y_pred_top10)\n",
    "print(\"RMSE:\", RMSE_top_10)\n",
    "print(\"R²:\", R2_top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf28a4",
   "metadata": {},
   "source": [
    "### 8. Plot the model comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf140df",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_models = ['Full Model', 'Top 15 Features', 'Top 10 Features']\n",
    "rmse = [RMSE_full, RMSE_top_15, RMSE_top_10]\n",
    "r2 = [R2_full, R2_top_15, R2_top_10]\n",
    "\n",
    "# Set up subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4.5))\n",
    "\n",
    "# RMSE plot\n",
    "bars_rmse = ax[0].bar(xgb_models, rmse, color=['indianred', 'lightsteelblue', 'slategray'])\n",
    "ax[0].set_title('RMSE Comparison')\n",
    "ax[0].set_ylabel('RMSE')\n",
    "\n",
    "# Annotate RMSE bars\n",
    "for bar in bars_rmse:\n",
    "    height = bar.get_height()\n",
    "    ax[0].text(bar.get_x() + bar.get_width() / 2, height + 0.01, f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "\n",
    "# R² plot    \n",
    "bars_r2 = ax[1].bar(xgb_models, r2, color=['indianred', 'lightsteelblue', 'slategray'])\n",
    "ax[1].set_title('R² Score Comparison')\n",
    "ax[1].set_ylabel('R²')\n",
    "\n",
    "# Annotate R² bars\n",
    "for bar in bars_r2:\n",
    "    height = bar.get_height()\n",
    "    ax[1].text(bar.get_x() + bar.get_width() / 2, height + 0.001, f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Model performance comparison: feature selection (XGBoost)')\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/m_performance_across_features_xgboost.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87678b9c",
   "metadata": {},
   "source": [
    "### 9. Plot of the top 15 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_features = feature_importance_data.sort_values(by='Importance', ascending=False).head(15)\n",
    "\n",
    "top_15_features['Feature'] = top_15_features['Feature'].replace(rename_dict)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(top_15_features['Feature'], top_15_features['Importance'], color='lightsteelblue')\n",
    "plt.xlabel('Importance score')\n",
    "plt.title('Top 15 feature importances, XGBoost')\n",
    "plt.gca().invert_yaxis()  # Highest at top\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/feature_importances_top15.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d418bb1",
   "metadata": {},
   "source": [
    "### 10. SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d019a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(best_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a4810",
   "metadata": {},
   "source": [
    "Kernel tends to crash when running the code below, especially if the explainer is TreeExplainer. We use the simple Explainer instead. The type of the eplainer is \"tree\" anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap_values_2 = explainer.shap_values(X_test) \n",
    "#joblib.dump(shap_values_2, \"output/shap_values_2.joblib\") #Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a16fba",
   "metadata": {},
   "source": [
    "No need to run the code again, the object can be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load:\n",
    "shap_values_2 = joblib.load(\"output/shap_values_2.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b120a1a",
   "metadata": {},
   "source": [
    "The code below show feature importances based on SHAP values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d43b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_shap = np.abs(shap_values_2).mean(axis=0)\n",
    "feature_names = X_test.columns\n",
    "\n",
    "sorted_idx = np.argsort(mean_abs_shap)[::-1]\n",
    "sorted_names = feature_names[sorted_idx]\n",
    "sorted_shap = mean_abs_shap[sorted_idx]\n",
    "\n",
    "clean_names = [rename_dict.get(name, name) for name in sorted_names[:15]]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(clean_names[:15][::-1], sorted_shap[:15][::-1], color=\"slategray\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xlabel(\"Mean (SHAP value)\")\n",
    "plt.title(\"Top 15 feature importances, SHAP\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/SHAP_top15.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bdf0f",
   "metadata": {},
   "source": [
    "Performing the prediction using 15 and 10 best features based on SHAP values, as a comparison to the ones by XGBoost. Maybe performance with these features is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top 15 and 10 most important features?\n",
    "\n",
    "top_10_shap = sorted_names[:10].tolist()\n",
    "\n",
    "X_train_top10_shap = X_train[top_10_shap] # creating a subset of features for training and testing\n",
    "X_test_top10_shap = X_test[top_10_shap]\n",
    "\n",
    "best_model_2 = joblib.load('best_model_2.pkl')\n",
    "\n",
    "best_model_2.fit(X_train_top10_shap, y_train)\n",
    "\n",
    "y_pred_top10_shap = best_model_2.predict(X_test_top10_shap)\n",
    "\n",
    "top_15_shap = sorted_names[:15].tolist()\n",
    "\n",
    "X_train_top15_shap = X_train[top_15_shap] # creating a subset of features for training and testing\n",
    "X_test_top15_shap = X_test[top_15_shap]\n",
    "\n",
    "best_model_2 = joblib.load('best_model_2.pkl') # Just in case we need a new load\n",
    "\n",
    "best_model_2.fit(X_train_top15_shap, y_train)\n",
    "\n",
    "y_pred_top15_shap = best_model_2.predict(X_test_top15_shap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae7f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_top_15_shap = np.sqrt(mean_squared_error(y_test, y_pred_top15_shap))\n",
    "R2_top_15_shap = r2_score(y_test, y_pred_top15_shap)\n",
    "\n",
    "RMSE_top_10_shap = np.sqrt(mean_squared_error(y_test, y_pred_top10_shap))\n",
    "R2_top_10_shap = r2_score(y_test, y_pred_top10_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_shaps = ['Full Model', 'Top 15 Features', 'Top 10 Features']\n",
    "rmse = [RMSE_full, RMSE_top_15_shap, RMSE_top_10_shap]\n",
    "r2 = [R2_full, R2_top_15_shap, R2_top_10_shap]\n",
    "\n",
    "# Set up subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4.5))\n",
    "\n",
    "# RMSE plot\n",
    "bars_rmse = ax[0].bar(xgboost_shaps, rmse, color=['indianred', 'lightsteelblue', 'slategray'])\n",
    "ax[0].set_title('RMSE Comparison')\n",
    "ax[0].set_ylabel('RMSE')\n",
    "\n",
    "# Annotate RMSE bars\n",
    "for bar in bars_rmse:\n",
    "    height = bar.get_height()\n",
    "    ax[0].text(bar.get_x() + bar.get_width() / 2, height + 0.01, f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# R² plot\n",
    "bars_r2 = ax[1].bar(xgboost_shaps, r2, color=['indianred', 'lightsteelblue', 'slategray'])\n",
    "ax[1].set_title('R² Score Comparison')\n",
    "ax[1].set_ylabel('R²')\n",
    "\n",
    "# Annotate R² bars\n",
    "for bar in bars_r2:\n",
    "    height = bar.get_height()\n",
    "    ax[1].text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Model performance comparison: feature selection (SHAP)')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/m_performance_across_features_shap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3950b",
   "metadata": {},
   "source": [
    "### 11. Interpretation of SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903035d",
   "metadata": {},
   "source": [
    "To run the code below, the kernel needs to be restarted, unfortunately. Moreover, make sure to load sections 1-4.2 again, then the 1st and 3rd code-chunk of section 10. There will be less variables in the memory, and the code will run (or should run) without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(explanation[:, \"it004_\"], color = explanation, show = False, dot_size = 2)\n",
    "plt.ylabel(\"SHAP value: recent internet use [IT]\")\n",
    "plt.xlabel(\"Recent internet use\")\n",
    "\n",
    "cbar_ax = plt.gcf().axes[-1]\n",
    "cbar_ax.set_ylabel(\"\") \n",
    "cbar_ax.set_ylabel(\"Gender\") \n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/int_it_gender.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(explanation[:, \"dn042_\"], color = explanation, show = False, dot_size = 2)\n",
    "plt.ylabel(\"SHAP value: gender\")\n",
    "plt.xlabel(\"Gender\")\n",
    "\n",
    "cbar_ax = plt.gcf().axes[-1]\n",
    "cbar_ax.set_ylabel(\"\") \n",
    "cbar_ax.set_ylabel(\"Recent internet use\") \n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/int_gender_it.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23635207",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(explanation[:, \"country\"], color = explanation, show = False, dot_size = 2)\n",
    "plt.ylabel(\"SHAP value: country\")\n",
    "plt.xlabel(\"Country\")\n",
    "\n",
    "cbar_ax = plt.gcf().axes[-1]\n",
    "cbar_ax.set_ylabel(\"\") \n",
    "cbar_ax.set_ylabel(\"Gender\") \n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/int_country_gender.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(explanation[:, \"br039_\"], color = explanation, show = False, dot_size = 2)\n",
    "plt.ylabel(\"SHAP value: recent alcohol use\")\n",
    "plt.xlabel(\"Recent alcohol use\")\n",
    "\n",
    "cbar_ax = plt.gcf().axes[-1]\n",
    "cbar_ax.set_ylabel(\"\") \n",
    "cbar_ax.set_ylabel(\"Gender\") \n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/int_alcohol_gender.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(explanation[:, \"hc116d3\"], color = explanation, show = False, dot_size = 2)\n",
    "plt.ylabel(\"SHAP value: any private health insurance?\")\n",
    "plt.xlabel(\"Private health insurance\")\n",
    "\n",
    "cbar_ax = plt.gcf().axes[-1]\n",
    "cbar_ax.set_ylabel(\"\") \n",
    "cbar_ax.set_ylabel(\"Gender\") \n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/int_private_insurance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d227a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(explanation[:, \"hc116dno\"], color = explanation, show = False, dot_size = 2)\n",
    "plt.ylabel(\"SHAP value: any other health insurance?\")\n",
    "plt.xlabel(\"Other health insurance\")\n",
    "\n",
    "cbar_ax = plt.gcf().axes[-1]\n",
    "cbar_ax.set_ylabel(\"\") \n",
    "cbar_ax.set_ylabel(\"Gender\") \n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/int_other_insurance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesing - but not in the paper.\n",
    "\n",
    "weight_idx = explanation.feature_names.index(\"ph012_\")\n",
    "\n",
    "# Create mask for weights > 20\n",
    "mask = explanation.data[:, weight_idx] > 20\n",
    "\n",
    "# Filter the explanation object\n",
    "filtered_explanation = explanation[mask]\n",
    "\n",
    "\n",
    "shap.plots.scatter(filtered_explanation[:, \"ph012_\"], color=filtered_explanation, show=False, dot_size=3)\n",
    "plt.ylabel(\"SHAP value for respondent's weight\")\n",
    "plt.xlabel(\"Weight\")\n",
    "\n",
    "cbar_ax = plt.gcf().axes[-1]\n",
    "cbar_ax.set_ylabel(\"\") \n",
    "cbar_ax.set_ylabel(\"Gender\") \n",
    "#plt.savefig(\"/Users/justina/Desktop/Data Science/plots/int_weight_gender.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
